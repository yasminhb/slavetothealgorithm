# week 12... FINAL PRESENTATION AHHHHHH!!!

Let me give you a run through of what I presented...

#### Here is the working link:

yasminhb.github.io

### The Code

#### Facial Tracking

A quick run down and summary of my code (I cleaned it up a LOT):

*INSERT IMAGE OF FACIAL TRACKING*

This code was entirely created by Web Dev Simplified via:
*INSERT LINK*

It uses the face.api library, and was slightly remixed by me in order to fit my particular idea.

#### Emotion Detection

Emotion detection is included as part of the code by Web Dev Simplified (ADD NAME OF GUY), however, I created an array using objects and values in order to gather information and use it as an input.

This can be seen here:
*INSERT IMAGE*

Here I was able to define all of the emotions as numbers, meaning I could take the largest data set and connect it with my next function.

This can also be seen in my console (screen shot taken from a few weeks ago and is no longer available in the current console):

*INSERT IMAGE*

#### Background Images

As I hand drew all of the images, this part of the code was actually quite easy. I loaded all of the images in and then used the if{} function in order to be selective with what is shown and drawn to the screen (depending on the emotion shown by the user).

This is what loading the background images looks like in the code:

*INSERT IMAGE OF LOADING*

*INSERT IMAGE OF IF BACKGROUND = HAPPY*

#### Animations

In order to create the animations, I used the p5.play library. I was able to create the animations by ordering images (frames of my drawn animation) and using the animation() function. 

Similar to the background images, I used the if{} and else{} function in order to connect these animations with the users emotion (as well as the background).

You can see how I loaded and created the animations in the code below:

*INSERT IMAGE OF CODE*


### The Working Sketch

By combining all of the functions and inputs from above, I managed to create a working sketch: *the garden of pathetic fallacy!*

Once all of these functions are connected, the sketch works sort of like this:

*INSERT GIF BELOW OF WORKING SKETCH*

For reference, I have included my photobooth application on the side so you can see my face changing (and how the sketch works). 

#### *So what does the user experience exactly?*

The sketch first loads the landing page which gives them instructions and briefly explains the sketch, as well as how it works.

After clicking *ENTER* the user then enters the sketch and can interact with the garden by changing their facial expression. 

I've found it's really fun to try and find all seven of the garden states (as some emotions are harder to imitate than others). 

It's definitely a lot of fun and feel like it would especially be fun for children to play around with. 

Perhaps this is something I could expand upon?


